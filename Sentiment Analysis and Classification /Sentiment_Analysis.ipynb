{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 6: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, you'll need dataset:\n",
    "- `hw7_train.csv`: dataset fro training\n",
    "- `hw7_test.csv`: dataset for test\n",
    "\n",
    "A snippet of the dataset is given below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import string\n",
    "import nltk\n",
    "from sklearn import metrics\n",
    "from gensim.models import word2vec\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Getting ready for college. I had a good sleep....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>We are having a party now to have all the fami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@marC0110 ummm.. i see you.. and i really wann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>@saboteur1 Thanks for following  Much apprecia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Why eat at home? Picnic plans for today are al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>I hear a puppy crying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>@lollipop26 Hi! Is there any chance you know o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>@Grumpydev Probably stretch to a new iPhone ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>@AmberMVaughan I know, that's totally the down...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>Just got the furthest I've ever got on Sonic 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  Getting ready for college. I had a good sleep....\n",
       "1      1  We are having a party now to have all the fami...\n",
       "2      0  @marC0110 ummm.. i see you.. and i really wann...\n",
       "3      1  @saboteur1 Thanks for following  Much apprecia...\n",
       "4      1  Why eat at home? Picnic plans for today are al...\n",
       "5      0                             I hear a puppy crying \n",
       "6      0  @lollipop26 Hi! Is there any chance you know o...\n",
       "7      0  @Grumpydev Probably stretch to a new iPhone ca...\n",
       "8      0  @AmberMVaughan I know, that's totally the down...\n",
       "9      0  Just got the furthest I've ever got on Sonic 2..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train = pd.read_csv(\"hw6_train.csv\")\n",
    "test = pd.read_csv(\"hw6_test.csv\")\n",
    "\n",
    "train.head(10)\n",
    "#test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Unsupervised Sentiment Analysis\n",
    "\n",
    "Write a function `analyze_sentiment(docs, labels, th)` as follows:\n",
    "- Takes three inputs:\n",
    "   - `docs` : a list of documents, \n",
    "   - `labels` the ground-truth sentiment labels of `docs`\n",
    "   - `th`: compound threshold\n",
    "- Use Vader to get a compound score of for each document in `docs`.  \n",
    "- If `compound score > th`, then the predicted label is 1; otherwise 0\n",
    "- Print out the classification report\n",
    "- This function has no returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(docs, labels, th=0):\n",
    "    pred_label = []\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    docs = docs.tolist()\n",
    "    for i in range(len(docs)):\n",
    "        sentiment_score = sia.polarity_scores(docs[i])\n",
    "        if sentiment_score['compound'] > th:\n",
    "            pred_label.append(1)\n",
    "        else: \n",
    "            pred_label.append(0)\n",
    "    #print(pred_label[:10])\n",
    "    \n",
    "    print(metrics.classification_report(labels, pred_label))\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.68      0.66      9968\n",
      "           1       0.66      0.62      0.64     10032\n",
      "\n",
      "    accuracy                           0.65     20000\n",
      "   macro avg       0.65      0.65      0.65     20000\n",
      "weighted avg       0.65      0.65      0.65     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analyze_sentiment(test[\"text\"], test[\"label\"], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Supervised Sentiment Analysis Using Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1: Train Word Vectors\n",
    "\n",
    "Write a function `train_wordvec(docs, vector_size)` as follows:\n",
    "- Take two inputs:\n",
    "    - `docs`: a list of documents\n",
    "    - `vector_size`: the dimension of word vectors\n",
    "- First tokenize `docs` into tokens\n",
    "- Use `gensim` package to train word vectors. Set the `vector size` and also carefully set other parameters such as `window`, `min_count` etc.\n",
    "- return the trained word vector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wordvec(docs, vector_size = 100):\n",
    "    \n",
    "    wv_model = None\n",
    "    sentences=[ [token.strip(string.punctuation).strip() \\\n",
    "             for token in nltk.word_tokenize(doc.lower()) \\\n",
    "                 if token not in string.punctuation and \\\n",
    "                 len(token.strip(string.punctuation).strip())>=2]\\\n",
    "             for doc in docs]\n",
    "    \n",
    "    \n",
    "    # print out tracking information\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                    level=logging.INFO)\n",
    "    \n",
    "    wv_model = word2vec.Word2Vec(sentences, \\\n",
    "            min_count=5, vector_size = vector_size, \\\n",
    "            window=5, workers=4 )\n",
    "    \n",
    "    return wv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 22:58:06,275 : INFO : collecting all words and their counts\n",
      "2022-12-18 22:58:06,275 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-12-18 22:58:06,289 : INFO : PROGRESS: at sentence #10000, processed 124321 words, keeping 18776 word types\n",
      "2022-12-18 22:58:06,304 : INFO : PROGRESS: at sentence #20000, processed 248579 words, keeping 31136 word types\n",
      "2022-12-18 22:58:06,319 : INFO : PROGRESS: at sentence #30000, processed 373660 words, keeping 41982 word types\n",
      "2022-12-18 22:58:06,334 : INFO : PROGRESS: at sentence #40000, processed 497842 words, keeping 51722 word types\n",
      "2022-12-18 22:58:06,350 : INFO : PROGRESS: at sentence #50000, processed 622677 words, keeping 61076 word types\n",
      "2022-12-18 22:58:06,366 : INFO : collected 69759 word types from a corpus of 746531 raw words and 60000 sentences\n",
      "2022-12-18 22:58:06,367 : INFO : Creating a fresh vocabulary\n",
      "2022-12-18 22:58:06,388 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 7830 unique words (11.22% of original 69759, drops 61929)', 'datetime': '2022-12-18T22:58:06.388740', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-13.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-12-18 22:58:06,389 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 667617 word corpus (89.43% of original 746531, drops 78914)', 'datetime': '2022-12-18T22:58:06.389266', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-13.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-12-18 22:58:06,406 : INFO : deleting the raw counts dictionary of 69759 items\n",
      "2022-12-18 22:58:06,407 : INFO : sample=0.001 downsamples 63 most-common words\n",
      "2022-12-18 22:58:06,408 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 525817.8138953568 word corpus (78.8%% of prior 667617)', 'datetime': '2022-12-18T22:58:06.408150', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-13.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-12-18 22:58:06,434 : INFO : estimated required memory for 7830 words and 100 dimensions: 10179000 bytes\n",
      "2022-12-18 22:58:06,435 : INFO : resetting layer weights\n",
      "2022-12-18 22:58:06,438 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-12-18T22:58:06.438607', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-13.0.1-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2022-12-18 22:58:06,439 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 7830 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-12-18T22:58:06.439048', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-13.0.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2022-12-18 22:58:06,616 : INFO : EPOCH 0: training on 746531 raw words (526112 effective words) took 0.2s, 3050029 effective words/s\n",
      "2022-12-18 22:58:06,814 : INFO : EPOCH 1: training on 746531 raw words (525488 effective words) took 0.2s, 2707253 effective words/s\n",
      "2022-12-18 22:58:07,008 : INFO : EPOCH 2: training on 746531 raw words (526050 effective words) took 0.2s, 2769044 effective words/s\n",
      "2022-12-18 22:58:07,210 : INFO : EPOCH 3: training on 746531 raw words (526159 effective words) took 0.2s, 2647977 effective words/s\n",
      "2022-12-18 22:58:07,395 : INFO : EPOCH 4: training on 746531 raw words (526038 effective words) took 0.2s, 2922034 effective words/s\n",
      "2022-12-18 22:58:07,396 : INFO : Word2Vec lifecycle event {'msg': 'training on 3732655 raw words (2629847 effective words) took 1.0s, 2747714 effective words/s', 'datetime': '2022-12-18T22:58:07.396388', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-13.0.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2022-12-18 22:58:07,396 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=7830, vector_size=100, alpha=0.025>', 'datetime': '2022-12-18T22:58:07.396740', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-13.0.1-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=7830, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "wv_model = train_wordvec(train[\"text\"], vector_size = 100)\n",
    "print(wv_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2: Generate Vector Representation for Documents\n",
    "\n",
    "Write a function `generate_doc_vector(docs, wv_model)` as follows:\n",
    "- Take two inputs:\n",
    "    - `docs`: a list of documents, \n",
    "    - `wv_model`: trained word vector model. Set the default value to 100.\n",
    "- First tokenize each document `doc` in `docs` into tokens\n",
    "- For each token in `doc`, look up for its word vector in `wv_model`. Then the document vector (denoted as `d`) of `doc` can be calculated as the `mean of the word vectors of its tokens`, i.e. $d = \\frac{\\sum_{i \\in doc}{v_i}}{|doc|}$, where $v_i$ is the word vector of the i-th token.\n",
    "- Return the vector representations of all `docs` as a numpy array of shape `(n, vector_size)`, where `n` is the number of documents in `docs` and `vector_size` is the dimension of word vectors.\n",
    "\n",
    "\n",
    "Note: It may not be a good idea to represent a document as the mean of its word vectors. For example, if one word is positive and another is negative, the sum of the these two words may make the resulting vector is no longer sensitive to sentiment. You'll learn more advanced methods to generate document vector in deep learning courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_doc_vector(docs, wv_model):\n",
    "    \n",
    "    vectors = []\n",
    "    \n",
    "    sentences=[[token.strip(string.punctuation).strip() \\\n",
    "             for token in nltk.word_tokenize(doc.lower()) \\\n",
    "                 if token not in string.punctuation and \\\n",
    "                 len(token.strip(string.punctuation).strip())>=2]\\\n",
    "             for doc in docs]\n",
    "    \n",
    "    for sent in sentences:\n",
    "        token_vector = []\n",
    "        for word in sent:\n",
    "            if word in wv_model.wv.key_to_index:\n",
    "                token_vector.append(wv_model.wv[word])\n",
    "        vectors.append(token_vector)\n",
    "        \n",
    "    d = np.ndarray((len(vectors), 100))\n",
    "    \n",
    "    for i in range(len(vectors)):\n",
    "        d[i] = np.mean(vectors[i], axis = 0)\n",
    "        \n",
    "    print(d.shape)\n",
    "    \n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subhuatharva/miniconda3/envs/my-env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/subhuatharva/miniconda3/envs/my-env/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 100)\n",
      "(20000, 100)\n"
     ]
    }
   ],
   "source": [
    "train_X = generate_doc_vector(train[\"text\"], wv_model)\n",
    "test_X = generate_doc_vector(test[\"text\"], wv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.DataFrame(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 100)\n",
      "(20000, 100)\n"
     ]
    }
   ],
   "source": [
    "train_X = generate_doc_vector(train[\"text\"], wv_model)\n",
    "test_X = generate_doc_vector(test[\"text\"], wv_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3: Put everything together\n",
    "\n",
    "\n",
    "Define a function `predict_sentiment(train_text, train_label, test_text, test_label, vector_size = 100)` as follows:\n",
    "\n",
    "- Take the following inputs:\n",
    "    - `train_text, train_label`: a list of documents and their labels for training\n",
    "    - `test_text, test_label`: a list of documents and their labels for testing,\n",
    "    - `vector_size`: the dimension of word vectors. Set the default value to 100.\n",
    "- Call `train_wordvec(docs, vector_size)` to train a word vector model using `train_text`\n",
    "- Call `generate_doc_vector(docs, wv_model)` to generate vector representations (denoted as `train_X`) for documents in `train_text`. \n",
    "- Call `generate_doc_vector(docs, wv_model)` to generate vector representations (denoted as `test_X`) for each document in `test_text`\n",
    "- Fit a linear SVM model using `train_X` and `train_label`\n",
    "- Predict the label for `test_X` and print out classification report for the testing subset.\n",
    "- This function has no return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.4: Analysis\n",
    "\n",
    "- Compare the classification reports you obtain from Q1 and Q2.3. Which model performs better?\n",
    "- Why this model can achieve better performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(train_text, train_label, test_text, test_label, vector_size = 100):\n",
    "    \n",
    "    wv_model = train_wordvec(train_text, vector_size)\n",
    "    \n",
    "    train_X = generate_doc_vector(train_text, wv_model)\n",
    "    train_X = pd.DataFrame(train_X)\n",
    "    train_X = train_X.fillna(0)\n",
    "    \n",
    "    test_X = generate_doc_vector(test_text, wv_model)\n",
    "    test_X = pd.DataFrame(test_X)\n",
    "    test_X = test_X.fillna(0)\n",
    "    \n",
    "    classifier = LinearSVC()\n",
    "    classifier.fit(train_X, train_label)\n",
    "    \n",
    "    y_pred = classifier.predict(test_X)\n",
    "    \n",
    "    print(metrics.classification_report(test_label, y_pred))\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 22:58:25,864 : INFO : collecting all words and their counts\n",
      "2022-12-18 22:58:25,864 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-12-18 22:58:25,878 : INFO : PROGRESS: at sentence #10000, processed 124321 words, keeping 18776 word types\n",
      "2022-12-18 22:58:25,892 : INFO : PROGRESS: at sentence #20000, processed 248579 words, keeping 31136 word types\n",
      "2022-12-18 22:58:25,907 : INFO : PROGRESS: at sentence #30000, processed 373660 words, keeping 41982 word types\n",
      "2022-12-18 22:58:25,921 : INFO : PROGRESS: at sentence #40000, processed 497842 words, keeping 51722 word types\n",
      "2022-12-18 22:58:25,937 : INFO : PROGRESS: at sentence #50000, processed 622677 words, keeping 61076 word types\n",
      "2022-12-18 22:58:25,953 : INFO : collected 69759 word types from a corpus of 746531 raw words and 60000 sentences\n",
      "2022-12-18 22:58:25,954 : INFO : Creating a fresh vocabulary\n",
      "2022-12-18 22:58:25,975 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 7830 unique words (11.22% of original 69759, drops 61929)', 'datetime': '2022-12-18T22:58:25.975540', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-13.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-12-18 22:58:25,976 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 667617 word corpus (89.43% of original 746531, drops 78914)', 'datetime': '2022-12-18T22:58:25.976153', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-13.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-12-18 22:58:25,992 : INFO : deleting the raw counts dictionary of 69759 items\n",
      "2022-12-18 22:58:25,993 : INFO : sample=0.001 downsamples 63 most-common words\n",
      "2022-12-18 22:58:25,993 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 525817.8138953568 word corpus (78.8%% of prior 667617)', 'datetime': '2022-12-18T22:58:25.993751', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-13.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-12-18 22:58:26,018 : INFO : estimated required memory for 7830 words and 100 dimensions: 10179000 bytes\n",
      "2022-12-18 22:58:26,019 : INFO : resetting layer weights\n",
      "2022-12-18 22:58:26,022 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-12-18T22:58:26.022327', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-13.0.1-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2022-12-18 22:58:26,022 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 7830 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-12-18T22:58:26.022680', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-13.0.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2022-12-18 22:58:26,209 : INFO : EPOCH 0: training on 746531 raw words (525805 effective words) took 0.2s, 2888800 effective words/s\n",
      "2022-12-18 22:58:26,401 : INFO : EPOCH 1: training on 746531 raw words (525955 effective words) took 0.2s, 2805835 effective words/s\n",
      "2022-12-18 22:58:26,590 : INFO : EPOCH 2: training on 746531 raw words (525798 effective words) took 0.2s, 2829513 effective words/s\n",
      "2022-12-18 22:58:26,781 : INFO : EPOCH 3: training on 746531 raw words (525458 effective words) took 0.2s, 2807897 effective words/s\n",
      "2022-12-18 22:58:26,970 : INFO : EPOCH 4: training on 746531 raw words (526156 effective words) took 0.2s, 2864241 effective words/s\n",
      "2022-12-18 22:58:26,970 : INFO : Word2Vec lifecycle event {'msg': 'training on 3732655 raw words (2629172 effective words) took 0.9s, 2775184 effective words/s', 'datetime': '2022-12-18T22:58:26.970438', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-13.0.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2022-12-18 22:58:26,970 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=7830, vector_size=100, alpha=0.025>', 'datetime': '2022-12-18T22:58:26.970639', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-13.0.1-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 100)\n",
      "(20000, 100)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.70      0.71      9968\n",
      "           1       0.71      0.73      0.72     10032\n",
      "\n",
      "    accuracy                           0.71     20000\n",
      "   macro avg       0.71      0.71      0.71     20000\n",
      "weighted avg       0.71      0.71      0.71     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subhuatharva/miniconda3/envs/my-env/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(train[\"text\"], train[\"label\"],\\\n",
    "                  test[\"text\"], test[\"label\"],\\\n",
    "                  vector_size = 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Q1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.68      0.66      9968\n",
      "           1       0.66      0.62      0.64     10032\n",
      "\n",
      "    accuracy                           0.65     20000\n",
      "   macro avg       0.65      0.65      0.65     20000\n",
      "weighted avg       0.65      0.65      0.65     20000\n",
      "\n",
      "\n",
      "==================\n",
      "\n",
      "Test Q2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 22:59:04,423 : INFO : collecting all words and their counts\n",
      "2022-12-18 22:59:04,424 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-12-18 22:59:04,437 : INFO : PROGRESS: at sentence #10000, processed 124321 words, keeping 18776 word types\n",
      "2022-12-18 22:59:04,452 : INFO : PROGRESS: at sentence #20000, processed 248579 words, keeping 31136 word types\n",
      "2022-12-18 22:59:04,467 : INFO : PROGRESS: at sentence #30000, processed 373660 words, keeping 41982 word types\n",
      "2022-12-18 22:59:04,482 : INFO : PROGRESS: at sentence #40000, processed 497842 words, keeping 51722 word types\n",
      "2022-12-18 22:59:04,497 : INFO : PROGRESS: at sentence #50000, processed 622677 words, keeping 61076 word types\n",
      "2022-12-18 22:59:04,512 : INFO : collected 69759 word types from a corpus of 746531 raw words and 60000 sentences\n",
      "2022-12-18 22:59:04,512 : INFO : Creating a fresh vocabulary\n",
      "2022-12-18 22:59:04,535 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 7830 unique words (11.22% of original 69759, drops 61929)', 'datetime': '2022-12-18T22:59:04.535354', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-13.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-12-18 22:59:04,535 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 667617 word corpus (89.43% of original 746531, drops 78914)', 'datetime': '2022-12-18T22:59:04.535827', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-13.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-12-18 22:59:04,551 : INFO : deleting the raw counts dictionary of 69759 items\n",
      "2022-12-18 22:59:04,552 : INFO : sample=0.001 downsamples 63 most-common words\n",
      "2022-12-18 22:59:04,553 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 525817.8138953568 word corpus (78.8%% of prior 667617)', 'datetime': '2022-12-18T22:59:04.553263', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-13.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-12-18 22:59:04,578 : INFO : estimated required memory for 7830 words and 100 dimensions: 10179000 bytes\n",
      "2022-12-18 22:59:04,579 : INFO : resetting layer weights\n",
      "2022-12-18 22:59:04,581 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-12-18T22:59:04.581972', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-13.0.1-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2022-12-18 22:59:04,582 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 7830 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-12-18T22:59:04.582336', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-13.0.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2022-12-18 22:59:04,762 : INFO : EPOCH 0: training on 746531 raw words (525942 effective words) took 0.2s, 3014824 effective words/s\n",
      "2022-12-18 22:59:04,943 : INFO : EPOCH 1: training on 746531 raw words (525759 effective words) took 0.2s, 2962348 effective words/s\n",
      "2022-12-18 22:59:05,121 : INFO : EPOCH 2: training on 746531 raw words (525780 effective words) took 0.2s, 3002577 effective words/s\n",
      "2022-12-18 22:59:05,306 : INFO : EPOCH 3: training on 746531 raw words (525510 effective words) took 0.2s, 2902204 effective words/s\n",
      "2022-12-18 22:59:05,485 : INFO : EPOCH 4: training on 746531 raw words (525506 effective words) took 0.2s, 3001281 effective words/s\n",
      "2022-12-18 22:59:05,486 : INFO : Word2Vec lifecycle event {'msg': 'training on 3732655 raw words (2628497 effective words) took 0.9s, 2909385 effective words/s', 'datetime': '2022-12-18T22:59:05.486083', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-13.0.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2022-12-18 22:59:05,486 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=7830, vector_size=100, alpha=0.025>', 'datetime': '2022-12-18T22:59:05.486282', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-13.0.1-arm64-arm-64bit', 'event': 'created'}\n",
      "/Users/subhuatharva/miniconda3/envs/my-env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/subhuatharva/miniconda3/envs/my-env/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 100)\n",
      "(20000, 100)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.71      0.71      9968\n",
      "           1       0.71      0.73      0.72     10032\n",
      "\n",
      "    accuracy                           0.72     20000\n",
      "   macro avg       0.72      0.72      0.72     20000\n",
      "weighted avg       0.72      0.72      0.72     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subhuatharva/miniconda3/envs/my-env/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    train = pd.read_csv(\"hw6_train.csv\")\n",
    "    test = pd.read_csv(\"hw6_test.csv\")\n",
    "    \n",
    "    # Test Q1\n",
    "    \n",
    "    print(\"Test Q1\")\n",
    "   \n",
    "    analyze_sentiment(test[\"text\"], test[\"label\"], 0)\n",
    "    \n",
    "    print(\"\\n==================\\n\")\n",
    "    print(\"Test Q2\")\n",
    "    \n",
    "    predict_sentiment(train[\"text\"], train[\"label\"],\\\n",
    "                  test[\"text\"], test[\"label\"],\\\n",
    "                  vector_size = 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
