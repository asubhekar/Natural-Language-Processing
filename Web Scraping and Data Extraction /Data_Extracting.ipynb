{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WE9WGE_m-4J-"
   },
   "source": [
    "# HW 4: Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">Each assignment needs to be completed independently. Never ever copy others' work or let someone copy your solution (even with minor modification, e.g. changing variable names). Anti-Plagiarism software will be used to check all submissions. No last minute extension of due date. Be sure to start working on it ASAP! </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKVEJOmf-4KC"
   },
   "source": [
    "## Q1: Extract data using regular expression\n",
    "Suppose you have scraped the text shown below from an online source (https://www.google.com/finance/). \n",
    "Define a `extract` function which:\n",
    "- takes a piece of text (in the format of shown below) as an input\n",
    "- uses regular expression to transform the text into a DataFrame with columns: 'Ticker','Name','Article','Media','Time','Price',and 'Change' \n",
    "- returns the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:27:54.060105Z",
     "start_time": "2021-10-19T07:27:50.568579Z"
    },
    "id": "Cq9Xzhnw-4KD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import sklearn\n",
    "import spacy\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''QQQ\n",
    "Invesco QQQ Trust Series 1\n",
    "Invesco Expands QQQ Innovation Suite to Include Small-Cap ETF\n",
    "PR Newswire • 4 hours ago\n",
    "$265.62\n",
    "1.13%\n",
    "add_circle_outline\n",
    "AAPL\n",
    "Apple Inc\n",
    "Estimating The Fair Value Of Apple Inc. (NASDAQ:AAPL)\n",
    "Yahoo Finance • 4 hours ago\n",
    "$140.41\n",
    "1.50%\n",
    "add_circle_outline\n",
    "TSLA\n",
    "Tesla Inc\n",
    "Could This Tesla Stock Unbalanced Iron Condor Return 23%?\n",
    "Investor's Business Daily • 1 hour ago\n",
    "$218.30\n",
    "0.49%\n",
    "add_circle_outline\n",
    "AMZN\n",
    "Amazon.com, Inc.\n",
    "The Regulators of Facebook, Google and Amazon Also Invest in the Companies' Stocks\n",
    "Wall Street Journal • 2 days ago\n",
    "$110.91\n",
    "1.76%\n",
    "add_circle_outline'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:27:54.082105Z",
     "start_time": "2021-10-19T07:27:54.072106Z"
    },
    "id": "StFfIbLB-4KG"
   },
   "outputs": [],
   "source": [
    "def extract(text):\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(columns = [\"Ticker\", \"Name\", \"Article\", \"Media\", \"Time\", \"Price\", \"Change\"])\n",
    "    \n",
    "    # split text\n",
    "    individual_companies = re.split('\\nadd_circle_outline', text)\n",
    "    individual_companies = individual_companies[0:len(individual_companies)-1]\n",
    "\n",
    "    for company in individual_companies:\n",
    "        company1 = company.split('\\n')\n",
    "        \n",
    "        \n",
    "        while '' in company1:\n",
    "            company1.remove('')\n",
    "            \n",
    "        #print(company1)\n",
    "        \n",
    "        company2 = []\n",
    "        for component in company1:\n",
    "            if '•' in component:\n",
    "                article_date = component.split('•')\n",
    "                company2.append(article_date[0])\n",
    "                company2.append(article_date[1])\n",
    "\n",
    "            else:\n",
    "                company2.append(component)\n",
    "        print(company2)\n",
    "        df.loc[len(df)] = company2\n",
    "            \n",
    "    result = df\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:27:54.094108Z",
     "start_time": "2021-10-19T07:27:54.084106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['QQQ', 'Invesco QQQ Trust Series 1', 'Invesco Expands QQQ Innovation Suite to Include Small-Cap ETF', 'PR Newswire ', ' 4 hours ago', '$265.62', '1.13%']\n",
      "['AAPL', 'Apple Inc', 'Estimating The Fair Value Of Apple Inc. (NASDAQ:AAPL)', 'Yahoo Finance ', ' 4 hours ago', '$140.41', '1.50%']\n",
      "['TSLA', 'Tesla Inc', 'Could This Tesla Stock Unbalanced Iron Condor Return 23%?', \"Investor's Business Daily \", ' 1 hour ago', '$218.30', '0.49%']\n",
      "['AMZN', 'Amazon.com, Inc.', \"The Regulators of Facebook, Google and Amazon Also Invest in the Companies' Stocks\", 'Wall Street Journal ', ' 2 days ago', '$110.91', '1.76%']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Name</th>\n",
       "      <th>Article</th>\n",
       "      <th>Media</th>\n",
       "      <th>Time</th>\n",
       "      <th>Price</th>\n",
       "      <th>Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QQQ</td>\n",
       "      <td>Invesco QQQ Trust Series 1</td>\n",
       "      <td>Invesco Expands QQQ Innovation Suite to Includ...</td>\n",
       "      <td>PR Newswire</td>\n",
       "      <td>4 hours ago</td>\n",
       "      <td>$265.62</td>\n",
       "      <td>1.13%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>Estimating The Fair Value Of Apple Inc. (NASDA...</td>\n",
       "      <td>Yahoo Finance</td>\n",
       "      <td>4 hours ago</td>\n",
       "      <td>$140.41</td>\n",
       "      <td>1.50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>Tesla Inc</td>\n",
       "      <td>Could This Tesla Stock Unbalanced Iron Condor ...</td>\n",
       "      <td>Investor's Business Daily</td>\n",
       "      <td>1 hour ago</td>\n",
       "      <td>$218.30</td>\n",
       "      <td>0.49%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com, Inc.</td>\n",
       "      <td>The Regulators of Facebook, Google and Amazon ...</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>$110.91</td>\n",
       "      <td>1.76%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ticker                        Name  \\\n",
       "0    QQQ  Invesco QQQ Trust Series 1   \n",
       "1   AAPL                   Apple Inc   \n",
       "2   TSLA                   Tesla Inc   \n",
       "3   AMZN            Amazon.com, Inc.   \n",
       "\n",
       "                                             Article  \\\n",
       "0  Invesco Expands QQQ Innovation Suite to Includ...   \n",
       "1  Estimating The Fair Value Of Apple Inc. (NASDA...   \n",
       "2  Could This Tesla Stock Unbalanced Iron Condor ...   \n",
       "3  The Regulators of Facebook, Google and Amazon ...   \n",
       "\n",
       "                        Media          Time    Price Change  \n",
       "0                PR Newswire    4 hours ago  $265.62  1.13%  \n",
       "1              Yahoo Finance    4 hours ago  $140.41  1.50%  \n",
       "2  Investor's Business Daily     1 hour ago  $218.30  0.49%  \n",
       "3        Wall Street Journal     2 days ago  $110.91  1.76%  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your function\n",
    "\n",
    "extract(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aab-6OWh-4KI"
   },
   "source": [
    "## Q2: Analyze a document\n",
    "\n",
    "When you have a long document, you would like to \n",
    "- Quanitfy how `concrete` a sentence is\n",
    "- Create a concise summary while preserving it's key information content and overall meaning. Let's implement an `extractive method` based on the concept of TF-IDF. The idea is to identify the key sentences from an article and use them as a summary. \n",
    "\n",
    "\n",
    "Carefully follow the following steps to achieve these two targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsnCv2VZ-4KI"
   },
   "source": [
    "### Q2.1. Preprocess the input document \n",
    "\n",
    "Define a function `proprocess(doc, lemmatized = True, remove_stopword = True, lower_case = True, remove_punctuation = True, pos_tag = False)` \n",
    "- Four input parameters:\n",
    "    - `doc`: an input string (e.g. a document)\n",
    "    - `lemmatized`: an optional boolean parameter to indicate if tokens are lemmatized. The default value is True (i.e. tokens are lemmatized).\n",
    "    - `remove_stopword`: an optional boolean parameter to remove stop words. The default value is True, i.e., remove stop words. \n",
    "    - `remove_punctuation`: optional boolean parameter to remove punctuations. The default values is True, i.e., remove all punctuations.\n",
    "    - `lower_case`: optional boolean parameter to convert all tokens to lower case. The default option is True, i.e., lowercase all tokens.\n",
    "    - `pos_tag`: optional boolean parameter to add a POS tag for each token. The default option is False, i.e., no POS tagging.  \n",
    "    \n",
    "       \n",
    "- Split the input `doc` into sentences. Hint, typically, `\\n\\n+` is used to separate paragraphs. Make sure a sentence does not cross over two paragraphs. You can replace `\\n\\n+` by a `.`\n",
    "\n",
    "\n",
    "- Tokenize each sentence into unigram tokens and also process the tokens as follows:\n",
    "    - If `lemmatized` is True, lemmatize all unigrams. \n",
    "    - If `remove_stopword` is set to True, remove all stop words. \n",
    "    - If `remove_punctuation` is set to True, remove all punctuations. \n",
    "    - If `lower_case` is set to True, convert all tokens to lower case \n",
    "    - If `pos_tag` is set to True, find the POS tag for each token and form a tuple for each token, e.g., ('recently', 'ADV'). Either Penn tags or Universal tags are fine. See mapping of these two tagging systems here: https://universaldependencies.org/tagset-conversion/en-penn-uposf.html\n",
    "\n",
    "\n",
    "- Return the original sentence list (`sents`) and also the tokenized (or tagged) sentence list (`tokenized_sents`). \n",
    "\n",
    "   \n",
    "(Hint: you can use [nltk](https://www.nltk.org/api/nltk.html) and [spacy](https://spacy.io/api/token#attributes) package for this task.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:27:54.556104Z",
     "start_time": "2021-10-19T07:27:54.096107Z"
    },
    "id": "A9gBjG6V-4KK"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess(doc, lemmatized=True, pos_tag = False, remove_stopword=True, lower_case = True, remove_punctuation = True):\n",
    "\n",
    "    # output variables\n",
    "    sents, tokenized_sents = None, []\n",
    "    \n",
    "    # construct a language object\n",
    "    doc = nlp(doc)\n",
    "    doc_raw = str(doc)\n",
    "    doc_raw = re.split('\\n\\n+', doc_raw)\n",
    "    doc_raw = ' '.join(doc_raw)\n",
    "    sents_raw = nltk.sent_tokenize(doc_raw)\n",
    "    \n",
    "    # lemmatization\n",
    "    if lemmatized == True:\n",
    "        doc = \" \".join([token.lemma_ for token in doc])\n",
    "    else:\n",
    "        doc = str(doc)\n",
    "        doc = re.split('\\n\\n+', doc)\n",
    "        doc = ' '.join(doc)\n",
    "        \n",
    "    if lower_case == True:\n",
    "        doc = doc.lower()\n",
    "    \n",
    "    # tokenizing sentences\n",
    "    sents = nltk.sent_tokenize(doc)\n",
    "    \n",
    "    # removing punctuation\n",
    "    sents2 = []\n",
    "    if remove_punctuation == True:\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        for sent in sents:\n",
    "            token = tokenizer.tokenize(sent)\n",
    "            sent = ' '.join(token)\n",
    "            sents2.append(sent)\n",
    "    else:\n",
    "        sents2 = sents\n",
    "        \n",
    "\n",
    "    \n",
    "    # removing stopword\n",
    "    tokenized_sents2 = []\n",
    "    if remove_stopword == True:\n",
    "        stop_words = nlp.Defaults.stop_words\n",
    "        for sentence in sents2:\n",
    "            tokenized_sents.append(nltk.word_tokenize(sentence))\n",
    "        \n",
    "        for group in tokenized_sents:\n",
    "            filtered_sentence = [w for w in group if not w in stop_words]\n",
    "            tokenized_sents2.append(filtered_sentence)\n",
    "        \n",
    "    else:\n",
    "        for sentence in sents2:\n",
    "            tokenized_sents2.append(nltk.word_tokenize(sentence))\n",
    "    \n",
    "    # position_tagging\n",
    "    tokenized_sents_final = []\n",
    "    if pos_tag == True:\n",
    "        for sent in sents_raw:\n",
    "            sent = nlp(sent)\n",
    "            tokenized_sents_final.append([(str(token), token.pos_) for token in sent])\n",
    "    else:\n",
    "        tokenized_sents_final = tokenized_sents2\n",
    "        \n",
    "    \n",
    "    # returning the results\n",
    "    return sents_raw, tokenized_sents_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:27:54.566108Z",
     "start_time": "2021-10-19T07:27:54.558107Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kenx0JgK-4KL",
    "outputId": "309988ea-efde-4cd6-ed0b-45ad8a566b26"
   },
   "outputs": [],
   "source": [
    "# load test document\n",
    "\n",
    "text = open(\"power_of_nlp.txt\", \"r\", encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:27:56.835401Z",
     "start_time": "2021-10-19T07:27:54.568105Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "drwiHlEb-4KL",
    "outputId": "fe258009-9ff4-43c4-eb71-1a2876e6084d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Power of Natural Language Processing Until recently, the conventional wisdom was that while AI was better than humans at data-driven decision making tasks, it was still inferior to humans for cognitive and creative ones. \n",
      " ['power', 'natural', 'language', 'processing', 'recently', 'conventional', 'wisdom', 'ai', 'human', 'data', 'drive', 'decision', 'task', 'inferior', 'human', 'cognitive', 'creative'] \n",
      "\n",
      "\n",
      "But in the past two years language-based AI has advanced by leaps and bounds, changing common notions of what this technology can do. \n",
      " ['past', 'year', 'language', 'base', 'ai', 'advance', 'leap', 'bound', 'change', 'common', 'notion', 'technology'] \n",
      "\n",
      "\n",
      "The most visible advances have been in what’s called “natural language processing” (NLP), the branch of AI focused on how computers can process language like humans do. \n",
      " ['visible', 'advance', 's', 'natural', 'language', 'processing', 'nlp', 'branch', 'ai', 'focus', 'computer', 'process', 'language', 'like', 'human'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test with all default options:\n",
    "\n",
    "sents, tokenized_sents = preprocess(text)\n",
    "\n",
    "# print first 3 sentences\n",
    "for i in range(3):\n",
    "    print(sents[i], \"\\n\",tokenized_sents[i],\"\\n\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Power of Natural Language Processing Until recently, the conventional wisdom was that while AI was better than humans at data-driven decision making tasks, it was still inferior to humans for cognitive and creative ones. \n",
      " [('The', 'DET'), ('Power', 'PROPN'), ('of', 'ADP'), ('Natural', 'PROPN'), ('Language', 'PROPN'), ('Processing', 'NOUN'), ('Until', 'ADP'), ('recently', 'ADV'), (',', 'PUNCT'), ('the', 'DET'), ('conventional', 'ADJ'), ('wisdom', 'NOUN'), ('was', 'AUX'), ('that', 'SCONJ'), ('while', 'SCONJ'), ('AI', 'PROPN'), ('was', 'AUX'), ('better', 'ADJ'), ('than', 'ADP'), ('humans', 'NOUN'), ('at', 'ADP'), ('data', 'NOUN'), ('-', 'PUNCT'), ('driven', 'VERB'), ('decision', 'NOUN'), ('making', 'VERB'), ('tasks', 'NOUN'), (',', 'PUNCT'), ('it', 'PRON'), ('was', 'AUX'), ('still', 'ADV'), ('inferior', 'ADJ'), ('to', 'ADP'), ('humans', 'NOUN'), ('for', 'ADP'), ('cognitive', 'ADJ'), ('and', 'CCONJ'), ('creative', 'ADJ'), ('ones', 'NOUN'), ('.', 'PUNCT')] \n",
      "\n",
      "\n",
      "But in the past two years language-based AI has advanced by leaps and bounds, changing common notions of what this technology can do. \n",
      " [('But', 'CCONJ'), ('in', 'ADP'), ('the', 'DET'), ('past', 'ADJ'), ('two', 'NUM'), ('years', 'NOUN'), ('language', 'NOUN'), ('-', 'PUNCT'), ('based', 'VERB'), ('AI', 'PROPN'), ('has', 'AUX'), ('advanced', 'VERB'), ('by', 'ADP'), ('leaps', 'NOUN'), ('and', 'CCONJ'), ('bounds', 'NOUN'), (',', 'PUNCT'), ('changing', 'VERB'), ('common', 'ADJ'), ('notions', 'NOUN'), ('of', 'ADP'), ('what', 'PRON'), ('this', 'DET'), ('technology', 'NOUN'), ('can', 'AUX'), ('do', 'AUX'), ('.', 'PUNCT')] \n",
      "\n",
      "\n",
      "The most visible advances have been in what’s called “natural language processing” (NLP), the branch of AI focused on how computers can process language like humans do. \n",
      " [('The', 'DET'), ('most', 'ADV'), ('visible', 'ADJ'), ('advances', 'NOUN'), ('have', 'AUX'), ('been', 'AUX'), ('in', 'ADP'), ('what', 'PRON'), ('’s', 'AUX'), ('called', 'VERB'), ('“', 'PUNCT'), ('natural', 'ADJ'), ('language', 'NOUN'), ('processing', 'NOUN'), ('”', 'PUNCT'), ('(', 'PUNCT'), ('NLP', 'PROPN'), (')', 'PUNCT'), (',', 'PUNCT'), ('the', 'DET'), ('branch', 'NOUN'), ('of', 'ADP'), ('AI', 'PROPN'), ('focused', 'VERB'), ('on', 'ADP'), ('how', 'SCONJ'), ('computers', 'NOUN'), ('can', 'AUX'), ('process', 'VERB'), ('language', 'NOUN'), ('like', 'SCONJ'), ('humans', 'NOUN'), ('do', 'VERB'), ('.', 'PUNCT')] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# process text without remove stopwords, punctuation, lowercase, but with pos tagging\n",
    "\n",
    "sents, tokenized_sents = preprocess(text, lemmatized = False, pos_tag = True, \n",
    "                                    remove_stopword=False, remove_punctuation = False, \n",
    "                                    lower_case = False)\n",
    "\n",
    "for i in range(3):\n",
    "    print(sents[i], \"\\n\",tokenized_sents[i],\"\\n\\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2. Quantify sentence concreteness\n",
    "\n",
    "\n",
    "`Concreteness` can increase a message's persuasion. The concreteness can be measured by the use of :\n",
    "- `article` (e.g., a, an, and the), \n",
    "- `adpositions` (e.g., in, at, of, on, etc), and\n",
    "- `quantifiers`, i.e., adjectives before nouns.\n",
    "\n",
    "\n",
    "Define a function `compute_concreteness(tagged_sent)` as follows:\n",
    "- Input argument is `tagged_sent`, a list with (token, pos_tag) tuples as shown above.\n",
    "- Find the three types of tokens: `articles`, `adposition`, and `quantifiers`.\n",
    "- Compute `concereness` score as:  `(the sum of the counts of the three types of tokens)/(total non-punctuation tokens)`.\n",
    "- return the concreteness score, articles, adposition, and quantifiers lists.\n",
    "\n",
    "\n",
    "Find the most concrete and the least concrete sentences from the article. \n",
    "\n",
    "\n",
    "Reference: Peer to Peer Lending: The Relationship Between Language Features, Trustworthiness, and Persuasion Success, https://socialmedialab.sites.stanford.edu/sites/g/files/sbiybj22976/files/media/file/larrimore-jacr-peer-to-peer.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_concreteness(tagged_sent):\n",
    "    \n",
    "    concreteness, articles, adpositions,quantifier = None, None, None, []\n",
    "    \n",
    "    articles = [tag for tag in tagged_sent if tag[1] == \"DET\"]\n",
    "    adpositions = [tag for tag in tagged_sent if tag[1] == \"ADP\"]\n",
    "    #quantifier = [tag for tag in tagged_sent if tag[1] == \"ADJ\"]\n",
    "    \n",
    "    for i in range(len(tagged_sent) - 1):\n",
    "        if tagged_sent[i][1] == \"ADJ\" and tagged_sent[i+1][1] == \"NOUN\":\n",
    "            quantifier.append(tagged_sent[i])\n",
    "    \n",
    "    punctuation_tokens = [tag for tag in tagged_sent if tag[1] == \"PUNCT\"]\n",
    "    non_punctuation_tokens_count = len(tagged_sent) - len(punctuation_tokens)\n",
    "    \n",
    "    concreteness = (len(articles) + len(adpositions) + len(quantifier)) / non_punctuation_tokens_count\n",
    "    \n",
    "    return concreteness, articles, adpositions,quantifier\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize with pos tag, without change the text much\n",
    "\n",
    "sents, tokenized_sents = preprocess(text, lemmatized = False, pos_tag = True, \n",
    "                                    remove_stopword=False, remove_punctuation = False, \n",
    "                                    lower_case = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'But in the past two years language-based AI has advanced by leaps and bounds, changing common notions of what this technology can do.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.25,\n",
       " [('the', 'DET'), ('this', 'DET')],\n",
       " [('in', 'ADP'), ('by', 'ADP'), ('of', 'ADP')],\n",
       " [('common', 'ADJ')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find concreteness score, articles, adpositions, and quantifiers in a sentence\n",
    "\n",
    "idx = 1    # sentence id\n",
    "x = tokenized_sents[idx]\n",
    "concreteness, articles, adpositions,quantifier = compute_concreteness(x)\n",
    "\n",
    "# show sentence\n",
    "sents[idx]\n",
    "# show result\n",
    "concreteness, articles, adpositions,quantifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most concerete sentence:  Large foundation models like GPT-3 exhibit abilities to generalize to a large number of tasks without any task-specific training., 0.450\n",
      "\n",
      "The least concerete sentence:  Remember that while current AI might not be poised to replace managers, managers who understand AI are poised to replace managers who don’t., 0.000\n"
     ]
    }
   ],
   "source": [
    "# Find the most concrete and the least concrete sentences from the article\n",
    "\n",
    "concrete = []\n",
    "\n",
    "for token in tokenized_sents:\n",
    "    score = compute_concreteness(token)[0]\n",
    "    concrete.append(score)\n",
    "    \n",
    "max_id = concrete.index(max(concrete))\n",
    "min_id = concrete.index(min(concrete))\n",
    "\n",
    "\n",
    "print (f\"The most concerete sentence:  {sents[max_id]}, {concrete[max_id]:.3f}\\n\")\n",
    "print (f\"The least concerete sentence:  {sents[min_id]}, {concrete[min_id]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFBMtpV7-4KM"
   },
   "source": [
    "### Q2.3. Generate TF-IDF representations for sentences \n",
    "\n",
    "Define a function `compute_tf_idf(sents, use_idf)` as follows: \n",
    "\n",
    "\n",
    "- Take the following two inputs:\n",
    "    - `sents`: tokenized sentences (without pos tagging) returned from Q2.1. These sentences form a corpus for you to calculate `TF-IDF` vectors.\n",
    "    - `use_idf`: if this option is true, return smoothed normalized `TF_IDF` vectors for all sentences; otherwise, just return normalized `TF` vector for each sentence.\n",
    "    \n",
    "    \n",
    "- Calculate `TF-IDF` vectors as shown in the lecture notes (Hint: you can slightly modify code segment 7.5 in NLP Lecture Notes (II) for this task)\n",
    "\n",
    "- Return the `TF-IDF` vectors  if `use_idf` is True.  Return the `TF` vectors if `use_idf` is False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:27:56.846403Z",
     "start_time": "2021-10-19T07:27:56.837402Z"
    },
    "id": "pq9vq7uP-4KM"
   },
   "outputs": [],
   "source": [
    "def compute_tf_idf(sents, use_idf = True, min_df = 1):\n",
    "    \n",
    "    tf_idf = None\n",
    "    \n",
    "    # dictionary of token count\n",
    "    docs_tokens={idx:nltk.FreqDist(group) \\\n",
    "             for idx,group in enumerate(sents)}\n",
    "    \n",
    "    # dataframe from dictionary\n",
    "    dtm=pd.DataFrame.from_dict(docs_tokens, orient=\"index\")\n",
    "    dtm=dtm.fillna(0)\n",
    "    dtm = dtm.sort_index(axis = 0)\n",
    "    \n",
    "    # get normalized term frequency (tf) matrix\n",
    "    tf=dtm.values\n",
    "    doc_len=tf.sum(axis=1)\n",
    "    tf=np.divide(tf, doc_len[:,None])\n",
    "    np.set_printoptions(precision=2)\n",
    "    \n",
    "    # get idf\n",
    "    df=np.where(tf>0,1,0)\n",
    "    idf=np.log(np.divide(len(tokenized_sents), \\\n",
    "        np.sum(df, axis=0)))+1\n",
    "\n",
    "    smoothed_idf=np.log(np.divide(len(tokenized_sents)+1, np.sum(df, axis=0)+1))+1\n",
    "    \n",
    "    # step 6. get tf-idf\n",
    "\n",
    "    s = tf*idf\n",
    "    tf_idf=normalize(tf*idf)\n",
    "\n",
    "    smoothed_tf_idf=normalize(tf*smoothed_idf)\n",
    "\n",
    "    \n",
    "    if use_idf == True:\n",
    "        return smoothed_tf_idf\n",
    "    else:\n",
    "        return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:27:57.344401Z",
     "start_time": "2021-10-19T07:27:56.848401Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBOzkpEk-4KN",
    "outputId": "8a38a63c-b55c-44f5-b2bf-a3b3b5a7d4c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78, 485)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test compute_tf_idf function\n",
    "\n",
    "sents, tokenized_sents = preprocess(text)\n",
    "tf_idf = compute_tf_idf(tokenized_sents, use_idf = True)\n",
    "\n",
    "# show shape of TF-IDF\n",
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEYffajS-4KN"
   },
   "source": [
    "### Q2.4. Identify key sentences as summary \n",
    "\n",
    "The basic idea is that, in a coherence article, all sentences should center around some key ideas. If we can identify a subset of sentences, denoted as $S_{key}$, which precisely capture the key ideas,  then $S_{key}$ can be used as a summary. Moreover, $S_{key}$ should have high similarity to all the other sentences on average, because all sentences are centered around the key ideas contained in $S_{key}$. Therefore, we can identify whether a sentence belongs to $S_{key}$ by its similarity to all the other sentences.\n",
    "\n",
    "\n",
    "Define a function `get_summary(tf_idf, sents, topN = 5)`  as follows:\n",
    "\n",
    "- This function takes three inputs:\n",
    "    - `tf_idf`: the TF-IDF vectors of all the sentences in a document\n",
    "    - `sents`: the original sentences corresponding to the TF-IDF vectors\n",
    "    - `topN`: the top N sentences in the generated summary\n",
    "\n",
    "- Steps:\n",
    "    1. Calculate the cosine similarity for every pair of TF-IDF vectors \n",
    "    1. For each sentence, calculate its average similarity to all the others \n",
    "    1. Select the sentences with the `topN` largest average similarity \n",
    "    1. Print the `topN` sentences index\n",
    "    1. Return these sentences as the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:29:47.949165Z",
     "start_time": "2021-10-19T07:29:47.945086Z"
    },
    "id": "gDqgTk8c-4KO"
   },
   "outputs": [],
   "source": [
    "def get_summary(tf_idf, sents, topN = 5):\n",
    "    \n",
    "    summary = None\n",
    "    \n",
    "    cosine_similarity = sklearn.metrics.pairwise.cosine_similarity(tf_idf)\n",
    "    \n",
    "    avg_sim = [x.mean() for x in cosine_similarity]\n",
    "    \n",
    "    sorted_avg_sim = sorted(avg_sim, reverse = True)\n",
    "    \n",
    "    top_N_indices = []\n",
    "    for val in sorted_avg_sim[0:topN]:\n",
    "        top_N_indices.append(avg_sim.index(val))\n",
    "    \n",
    "    summary = [sents[i] for i in top_N_indices]\n",
    "    \n",
    "    \n",
    "    return summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:29:50.689152Z",
     "start_time": "2021-10-19T07:29:48.264799Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sw5L48aK-4KO",
    "outputId": "1d5109ef-432f-4cf5-d0b6-46d1b08dc96e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models like GPT-3 are considered to be foundation models — an emerging AI research area — which also work for other types of data such as images and video. \n",
      "\n",
      "To begin preparing now, start understanding your text data assets and the variety of cognitive tasks involved in different roles in your organization. \n",
      "\n",
      "There is so much text data, and you don’t need advanced models like GPT-3 to extract its value. \n",
      "\n",
      "A Language-Based AI Research Assistant In my own work, I’ve been looking at how GPT-3-based tools can assist researchers in the research process. \n",
      "\n",
      "Begin incorporating new language-based AI tools for a variety of tasks to better understand their capabilities. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# put everything together and test with different options\n",
    "\n",
    "sents, tokenized_sents = preprocess(text)\n",
    "tf_idf = compute_tf_idf(tokenized_sents, use_idf = True)\n",
    "summary = get_summary(tf_idf, sents, topN = 5)\n",
    "\n",
    "for sent in summary:\n",
    "    print(sent,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please test summary generated under different configurations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l0bXLQG-4KO"
   },
   "source": [
    "### Q2.5. Analysis \n",
    "\n",
    "- Do you think the way to quantify concreteness makes sense? Any other thoughts to measure concreteness or abstractness? Share your ideas in pdf.\n",
    "\n",
    "\n",
    "- Do you think this method is able to generate a good summary? Any pros or cons have you observed? \n",
    "\n",
    "\n",
    "- Do these options `lemmatized, remove_stopword, remove_punctuation, use_idf` matter? \n",
    "- Why do you think these options matter or do not matter? \n",
    "- If these options matter, what are the best values for these options?\n",
    "\n",
    "\n",
    "Write your analysis as a pdf file. Be sure to provide some evidence from the output of each step to support your arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIyY8lIr-4KP"
   },
   "source": [
    "### Q2.5. (Bonus 3 points). \n",
    "\n",
    "\n",
    "- Can you think a way to improve this extractive summary method? Explain the method you propose for improvement,  implement it, use it to generate a new summary, and demonstrate what is improved in the new summary.\n",
    "\n",
    "\n",
    "- Or, you can research on some other extractive summary methods and implement one here. Compare it with the one you implemented in Q2.1-Q2.3 and show pros and cons of each method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main block to test all functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:29:53.023232Z",
     "start_time": "2021-10-19T07:29:50.691147Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEVWwkRW-4KP",
    "outputId": "cf1d3b08-f8e9-4332-b0ee-2d43b01ecfac",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================\n",
      "\n",
      "Test Q1\n",
      "  Ticker                        Name  \\\n",
      "0    QQQ  Invesco QQQ Trust Series 1   \n",
      "1   AAPL                   Apple Inc   \n",
      "2   TSLA                   Tesla Inc   \n",
      "3   AMZN            Amazon.com, Inc.   \n",
      "\n",
      "                                             Article  \\\n",
      "0  Invesco Expands QQQ Innovation Suite to Includ...   \n",
      "1  Estimating The Fair Value Of Apple Inc. (NASDA...   \n",
      "2  Could This Tesla Stock Unbalanced Iron Condor ...   \n",
      "3  The Regulators of Facebook, Google and Amazon ...   \n",
      "\n",
      "                        Media          Time    Price Change  \n",
      "0                PR Newswire    4 hours ago  $265.62  1.13%  \n",
      "1              Yahoo Finance    4 hours ago  $140.41  1.50%  \n",
      "2  Investor's Business Daily     1 hour ago  $218.30  0.49%  \n",
      "3        Wall Street Journal     2 days ago  $110.91  1.76%  \n",
      "\n",
      "==================\n",
      "\n",
      "Test Q2.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'But in the past two years language-based AI has advanced by leaps and bounds, changing common notions of what this technology can do.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.25,\n",
       " [('the', 'DET'), ('this', 'DET')],\n",
       " [('in', 'ADP'), ('by', 'ADP'), ('of', 'ADP')],\n",
       " [('common', 'ADJ')])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================\n",
      "\n",
      "Test Q2.2-2.4\n",
      "['Models like GPT-3 are considered to be foundation models — an emerging AI research area — which also work for other types of data such as images and video.', 'To begin preparing now, start understanding your text data assets and the variety of cognitive tasks involved in different roles in your organization.', 'There is so much text data, and you don’t need advanced models like GPT-3 to extract its value.', 'A Language-Based AI Research Assistant In my own work, I’ve been looking at how GPT-3-based tools can assist researchers in the research process.', 'Begin incorporating new language-based AI tools for a variety of tasks to better understand their capabilities.']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    \n",
    "    text=text = '''QQQ\n",
    "Invesco QQQ Trust Series 1\n",
    "Invesco Expands QQQ Innovation Suite to Include Small-Cap ETF\n",
    "PR Newswire • 4 hours ago\n",
    "$265.62\n",
    "1.13%\n",
    "add_circle_outline\n",
    "AAPL\n",
    "Apple Inc\n",
    "Estimating The Fair Value Of Apple Inc. (NASDAQ:AAPL)\n",
    "Yahoo Finance • 4 hours ago\n",
    "$140.41\n",
    "1.50%\n",
    "add_circle_outline\n",
    "TSLA\n",
    "Tesla Inc\n",
    "Could This Tesla Stock Unbalanced Iron Condor Return 23%?\n",
    "Investor's Business Daily • 1 hour ago\n",
    "$218.30\n",
    "0.49%\n",
    "add_circle_outline\n",
    "AMZN\n",
    "Amazon.com, Inc.\n",
    "The Regulators of Facebook, Google and Amazon Also Invest in the Companies' Stocks\n",
    "Wall Street Journal • 2 days ago\n",
    "$110.91\n",
    "1.76%\n",
    "add_circle_outline'''\n",
    "    \n",
    "    \n",
    "    print(\"\\n==================\\n\")\n",
    "    print(\"Test Q1\")\n",
    "    print(extract(text))\n",
    "    \n",
    "    print(\"\\n==================\\n\")\n",
    "    print(\"Test Q2.1\")\n",
    "    \n",
    "    text = open(\"power_of_nlp.txt\", \"r\", encoding='utf-8').read()\n",
    "    \n",
    "    sents, tokenized_sents = preprocess(text, lemmatized = False, pos_tag = True, \n",
    "                                    remove_stopword=False, remove_punctuation = False, \n",
    "                                    lower_case = False)\n",
    "    \n",
    "    idx = 1    # sentence id\n",
    "    x = tokenized_sents[idx]\n",
    "    concreteness, articles, adpositions,quantifier = compute_concreteness(x)\n",
    "\n",
    "    # show sentence\n",
    "    sents[idx]\n",
    "    # show result\n",
    "    concreteness, articles, adpositions,quantifier\n",
    "    \n",
    "    print(\"\\n==================\\n\")\n",
    "    print(\"Test Q2.2-2.4\")\n",
    "    sents, tokenized_sents = preprocess(text)\n",
    "    tf_idf = compute_tf_idf(tokenized_sents, use_idf = True)\n",
    "    summary = get_summary(tf_idf, sents, topN = 5)\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
