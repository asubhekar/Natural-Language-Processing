{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSlgGKtRS5Iy"
   },
   "source": [
    "# Assignment 3: Machine Translation with seq2seq models\n",
    "### Name : Atharv Subhekar\n",
    "### CWID : 20015840\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWojnXJwUjXo"
   },
   "outputs": [],
   "source": [
    "# Importing required librarie\n",
    "from datasets import load_dataset\n",
    "from transformers import MarianMTModel, MarianTokenizer, M2M100ForConditionalGeneration, M2M100Tokenizer,MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import sacrebleu\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eC4vGQpTIJ_"
   },
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDSbjE3DS1ze"
   },
   "outputs": [],
   "source": [
    "# Specifying the dataset name using the language codes\n",
    "dataset = load_dataset(\"Muennighoff/flores200\", 'eng_Latn-spa_Latn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mFj8CEwaczL"
   },
   "outputs": [],
   "source": [
    "# Loading the dataset and storing english and spanish sentences\n",
    "english = dataset['dev']['sentence_eng_Latn']\n",
    "english_100 = english[:100]\n",
    "english_20 = english[:20] # Using smaller dataset for easier computation\n",
    "\n",
    "spanish = dataset['dev']['sentence_spa_Latn']\n",
    "spanish_100 = spanish[:100]\n",
    "spanish_20 = spanish[:20] # Using smaller dataset for easier computation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18CXf2jVNCJC",
    "outputId": "0db4fe09-4f03-42a7-ed6b-26d7b1774953"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentence -  Lead researchers say this may bring early detection of cancer, tuberculosis, HIV and malaria to patients in low-income countries, where the survival rates for illnesses such as breast cancer can be half those of richer countries.\n",
      "Spanish Sentence -  Los principales investigadores principales sostienen que esto puede permitir la detección precoz del cáncer, la tuberculosis, el VIH y la malaria en pacientes de países de bajos recursos, donde la tasa de supervivencia de enfermedades como el cáncer de mama puede ser la mitad de la de los países más avanzados.\n"
     ]
    }
   ],
   "source": [
    "# English Sentence\n",
    "print('English Sentence - ',english[1])\n",
    "# Spanish Sentence\n",
    "print('Spanish Sentence - ',spanish[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrjDJP4VsKJ2"
   },
   "source": [
    "## Data Statistics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XWsAOh-pqsCn",
    "outputId": "1a16ec95-1a2d-449a-bbe5-380b3d5d820b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum length English Se|ntence =  6\n",
      "Maximum length English Sentence =  47\n",
      "Average length English Sentence =  22.21\n",
      "English Vocabulary size =  1237\n",
      "Minimum length Spanish Sentence =  8\n",
      "Maximum length Spanish Sentence =  60\n",
      "Average length Spanish Sentence =  27.33\n",
      "Spanish Vocabulary size =  1382\n"
     ]
    }
   ],
   "source": [
    "\n",
    "english_words = []\n",
    "spanish_words = []\n",
    "english_lengths = []\n",
    "spanish_lengths = []\n",
    "english_tokens = []\n",
    "spanish_tokens = []\n",
    "for i in range(len(english_100)):\n",
    "  # Splitting the sentences into words\n",
    "  words_en = english_100[i].split()\n",
    "  words_sp = spanish_100[i].split()\n",
    "  english_words.append(words_en)\n",
    "  spanish_words.append(words_sp)\n",
    "\n",
    "for i in range(len(english_words)):\n",
    "  # Storing lenghts of all the sentences\n",
    "  english_lengths.append(len(english_words[i]))\n",
    "  spanish_lengths.append(len(spanish_words[i]))\n",
    "  english_tokens.extend(english_words)\n",
    "  spanish_tokens.extend(spanish_words)\n",
    "\n",
    "# Storing all the unique words in the corpus to get word vocabulary\n",
    "vocab_eng = []\n",
    "vocab_spa = []\n",
    "for i in range(len(english_tokens)):\n",
    "  for token in english_tokens[i]:\n",
    "    if token not in vocab_eng:\n",
    "      vocab_eng.append(token)\n",
    "  for token in spanish_tokens[i]:\n",
    "    if token not in vocab_spa:\n",
    "      vocab_spa.append(token)\n",
    "\n",
    "# Printing data statistics\n",
    "print('Minimum length English Se|ntence = ', np.min(english_lengths))\n",
    "print('Maximum length English Sentence = ', np.max(english_lengths))\n",
    "print('Average length English Sentence = ', np.mean(english_lengths))\n",
    "print('English Vocabulary size = ',len(vocab_eng))\n",
    "print('Minimum length Spanish Sentence = ', np.min(spanish_lengths))\n",
    "print('Maximum length Spanish Sentence = ', np.max(spanish_lengths))\n",
    "print('Average length Spanish Sentence = ', np.mean(spanish_lengths))\n",
    "print('Spanish Vocabulary size = ',len(vocab_spa))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHMH2iZ-sRPf"
   },
   "source": [
    "## Models\n",
    "### OPUS-MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2kKKVbiuBJ6"
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer and model from Hugging Face\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")\n",
    "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQE-7Xkw9gct"
   },
   "outputs": [],
   "source": [
    "# Tokenizing english sentences and padding them.\n",
    "tokenized = tokenizer(english_20, return_tensors=\"pt\", padding=True)\n",
    "# Feeding the tokenized data into the model. Returns translated tokens\n",
    "translated = model.generate(**tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WK4v-a63RV44"
   },
   "outputs": [],
   "source": [
    "# Decoding the translated tokens using tokenizer class\n",
    "output_opusmt = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9yvTFJ7X5Ud"
   },
   "source": [
    "### M2M-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nksXn54-X499"
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer and model from Hugging Face\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGqC9tqYYXlw"
   },
   "outputs": [],
   "source": [
    "# Tokenizing english sentences and padding them.\n",
    "tokenizer.src_lang = 'en'\n",
    "tokenized = tokenizer(english_20, return_tensors=\"pt\", padding=True)\n",
    "# Feeding the tokenized data into the model. Returns translated tokens\n",
    "translated = model.generate(**tokenized,forced_bos_token_id=tokenizer.get_lang_id('es'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KTl2b2ic2iX"
   },
   "outputs": [],
   "source": [
    "# Decoding the translated tokens using tokenizer class\n",
    "output_m2m100 = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ly6x43k0ZRc2"
   },
   "source": [
    "### MBART 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucU5-k-oZaZ_"
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer and model from Hugging Face\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YJrj2-3dF0d"
   },
   "outputs": [],
   "source": [
    "# Tokenizing english sentences and padding them.\n",
    "tokenizer.src_lang = 'en_XX'\n",
    "tokenized = tokenizer(english_20, return_tensors=\"pt\", padding=True)\n",
    "# Feeding the tokenized data into the model. Returns translated tokens\n",
    "translated = model.generate(**tokenized, forced_bos_token_id=tokenizer.lang_code_to_id['es_XX'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ku0aYcQ-d70t"
   },
   "outputs": [],
   "source": [
    "# Decoding the translated tokens using tokenizer class\n",
    "output_mbart50 = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tY7oiVeNeFvg"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "7rVAXFb5RqHY"
   },
   "outputs": [],
   "source": [
    "# Functions for BLEU score.\n",
    "def calculate_bleu(reference, hypothesis):\n",
    "    bleu = sacrebleu.corpus_bleu(hypothesis, [reference])\n",
    "    return bleu.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qLqUAhgSOvE",
    "outputId": "2b37897e-1817-422c-8376-575e23b47612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPUS MT Bleu score =  34.33040263391583\n",
      "M2M 100 Bleu score =  30.528845556510973\n",
      "MBART 50 Bleu score =  27.503239179477948\n"
     ]
    }
   ],
   "source": [
    "# Passing the reference and translated sentence into the bleu function\n",
    "opus_bleu = calculate_bleu(output_opusmt, spanish_20)\n",
    "m2m_bleu = calculate_bleu(output_m2m100, spanish_20)\n",
    "mbart_bleu = calculate_bleu(output_mbart50, spanish_20)\n",
    "\n",
    "print('OPUS MT Bleu score = ', opus_bleu)\n",
    "print('M2M 100 Bleu score = ', m2m_bleu)\n",
    "print('MBART 50 Bleu score = ', mbart_bleu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpTv1XPiwjF9"
   },
   "source": [
    "## Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZO2F2ZTNwk5k",
    "outputId": "ed951d75-8c08-4aef-dd61-16ab7c414d06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentence :  \"Panama Papers\" is an umbrella term for roughly ten million documents from Panamanian law firm Mossack Fonseca, leaked to the press in spring 2016.\n",
      "Spanish Reference:  La expresión genérica «Papeles de Panamá» se refiere a los casi diez millones de documentos del despacho de abogados panameño Mossack Fonseca que se filtraron en los medios en la primavera de 2016.\n",
      "OPUSMT Translated:  \"Panamá Papers\" es un término general para aproximadamente diez millones de documentos del bufete de abogados panameño Mossack Fonseca, filtrados a la prensa en la primavera de 2016.\n",
      "M2M 100 Translated: \"Panama Papers\" es un término de sombra para aproximadamente diez millones de documentos de la firma de ley de Panamá Mossack Fonseca, que se lanzó a la prensa en la primavera de 2016.\n",
      "MBART 50 Translated: \"Panama Papers\" es un término de referencia para aproximadamente diez millones de documentos de la firma de abogados panameña Mossack Fonseca, que se difundieron a la prensa en primavera de 2016.\n"
     ]
    }
   ],
   "source": [
    "print('English Sentence : ',english_20[15])\n",
    "print('Spanish Reference: ',spanish_20[15])\n",
    "print('OPUSMT Translated: ',output_opusmt[15])\n",
    "print('M2M 100 Translated:',output_m2m100[15])\n",
    "print('MBART 50 Translated:',output_mbart50[15])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjnBiL7DpH65"
   },
   "source": [
    "## Conclusion\n",
    "OPUS-MT : The bleu score while using OPUS MT was 34.28 making it the best model. The translations were well aligned with the reference data.\n",
    "M2M-100 : The bleu score achieved was 30.50. Translations were not as good as OPUS MT. The execution was very slow and Colab runs out of memory.\n",
    "MBART-50 : Worst performing model with bleu of 27.47.\n",
    "\n",
    "Comparison between translations and reference:\n",
    "\n",
    "OPUS MT translation has the same meaning but the structure of the sentence is different than the reference.\n",
    "\n",
    "M2M 100 translation is also very close to OPUS MT translation with similar meaning with similar structure as OPUS MT. Although a few words were incorrect. For eg. OPUS MT - es un término general\n",
    "M2M100 - es un término de sombra(sombra means shadow)\n",
    "\n",
    "MBART 50 translation has more issues choosing the correct words compared to the other 2 models. The meaning of the sentence is not correct and the words like \"de sombra\" has been replaced with \"de referencia\" which has different meanings.\n",
    "\n",
    "\n",
    "Due to out of memory issue, I had to reduce the size of data to just 20 sentences."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
